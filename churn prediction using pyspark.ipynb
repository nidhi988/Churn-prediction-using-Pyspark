{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Disable warnings, set Matplotlib inline plotting and load Pandas package\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_data = sqlContext.read.load('F:\\churn prediction\\pyspark-churn-prediction-master\\data\\churn-bigml-80.csv',\n",
    "                          format='com.databricks.spark.csv',\n",
    "                          header='true',\n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_data = sqlContext.read.load('F:\\churn prediction\\pyspark-churn-prediction-master\\data/churn-bigml-20.csv',\n",
    "                          format='com.databricks.spark.csv',\n",
    "                          header='true',\n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By taking 5 rows of the CV_data variable and generating a Pandas DataFrame with them, we can get a display of what the rows look like.\n",
    "pd.DataFrame(CV_data.take(5), columns=CV_data.columns).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark DataFrames include some built-in functions for statistical processing. The describe() function performs summary statistics calculations on all numeric columns, and returns them as a DataFrame.\n",
    "\n",
    "CV_data.describe().toPandas().transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, we're use the Pandas library to examine correlations between the numeric columns by generating scatter plots of them.\n",
    "\n",
    "#For the Pandas workload, we don't want to pull the entire data set into the Spark driver, as that might exhaust the available RAM and throw an out-of-memory exception. Instead, we'll randomly sample a portion of the data (say 10%) to get a rough idea of how it looks.\n",
    "\n",
    "numeric_features = [t[0] for t in CV_data.dtypes if t[1] == 'int' or t[1] == 'double']\n",
    "\n",
    "sampled_data = CV_data.select(numeric_features).sample(False, 0.10).toPandas()\n",
    "\n",
    "axs = pd.plotting.scatter_matrix(sampled_data, figsize=(12, 12));\n",
    "\n",
    "# Rotate axis labels and remove axis ticks\n",
    "n = len(sampled_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It's obvious that there are several highly correlated fields, ie Total day minutes and Total day charge. Such correlated data won't be very beneficial for our model training runs, so we're going to remove them. We'll do so by dropping one column of each pair of correlated fields, along with the State and Area code columns.\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "\n",
    "binary_map = {'Yes':1.0, 'No':0.0, 'True':1.0, 'False':0.0}\n",
    "toNum = UserDefinedFunction(lambda k: binary_map[k], DoubleType())\n",
    "\n",
    "CV_data = CV_data.drop('State').drop('Area code') \\\n",
    "    .drop('Total day charge').drop('Total eve charge') \\\n",
    "    .drop('Total night charge').drop('Total intl charge') \\\n",
    "    .withColumn('Churn', toNum(CV_data['Churn'])) \\\n",
    "    .withColumn('International plan', toNum(CV_data['International plan'])) \\\n",
    "    .withColumn('Voice mail plan', toNum(CV_data['Voice mail plan'])).cache()\n",
    "\n",
    "final_test_data = final_test_data.drop('State').drop('Area code') \\\n",
    "    .drop('Total day charge').drop('Total eve charge') \\\n",
    "    .drop('Total night charge').drop('Total intl charge') \\\n",
    "    .withColumn('Churn', toNum(final_test_data['Churn'])) \\\n",
    "    .withColumn('International plan', toNum(final_test_data['International plan'])) \\\n",
    "    .withColumn('Voice mail plan', toNum(final_test_data['Voice mail plan'])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "def labelData(data):\n",
    "    # label: row[end], features: row[0:end-1]\n",
    "    return data.rdd.map(lambda row: LabeledPoint(row[-1], row[:-1]))\n",
    "\n",
    "training_data, testing_data = labelData(CV_data).randomSplit([0.8, 0.2])\n",
    "\n",
    "model = DecisionTree.trainClassifier(training_data, numClasses=2, maxDepth=2,\n",
    "                                     categoricalFeaturesInfo={1:2, 2:2},\n",
    "                                     impurity='gini', maxBins=32)\n",
    "\n",
    "print (model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTreeModel classifier of depth 2 with 7 nodes\n",
    "  If (feature 12 <= 3.0)\n",
    "   If (feature 4 <= 262.8)\n",
    "    Predict: 0.0\n",
    "   Else (feature 4 > 262.8)\n",
    "    Predict: 1.0\n",
    "  Else (feature 12 > 3.0)\n",
    "   If (feature 4 <= 169.9)\n",
    "    Predict: 1.0\n",
    "   Else (feature 4 > 169.9)\n",
    "    Predict: 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "def getPredictionsLabels(model, test_data):\n",
    "    predictions = model.predict(test_data.map(lambda r: r.features))\n",
    "    return predictions.zip(test_data.map(lambda r: r.label))\n",
    "\n",
    "def printMetrics(predictions_and_labels):\n",
    "    metrics = MulticlassMetrics(predictions_and_labels)\n",
    "    print 'Precision of True ', metrics.precision(1)\n",
    "    print 'Precision of False', metrics.precision(0)\n",
    "    print 'Recall of True    ', metrics.recall(1)\n",
    "    print 'Recall of False   ', metrics.recall(0)\n",
    "    print 'F-1 Score         ', metrics.fMeasure()\n",
    "    print 'Confusion Matrix\\n', metrics.confusionMatrix().toArray()\n",
    "\n",
    "predictions_and_labels = getPredictionsLabels(model, testing_data)\n",
    "\n",
    "printMetrics(predictions_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stratified sampling\n",
    "stratified_CV_data = CV_data.sampleBy('Churn', fractions={0: 388./2278, 1: 1.0}).cache()\n",
    "\n",
    "stratified_CV_data.groupby('Churn').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, testing_data = labelData(stratified_CV_data).randomSplit([0.8, 0.2])\n",
    "\n",
    "model = DecisionTree.trainClassifier(training_data, numClasses=2, maxDepth=2,\n",
    "                                     categoricalFeaturesInfo={1:2, 2:2},\n",
    "                                     impurity='gini', maxBins=32)\n",
    "\n",
    "predictions_and_labels = getPredictionsLabels(model, testing_data)\n",
    "printMetrics(predictions_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision of True  0.958333333333\n",
    "#Precision of False 0.738636363636\n",
    "#Recall of True     0.75\n",
    "#Recall of False    0.955882352941\n",
    "#F-1 Score          0.8375\n",
    "#Confusion Matrix\n",
    "[[ 65\\.   3.]\n",
    " [ 23\\.  69.]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pipelining and model selection\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def vectorizeData(data):\n",
    "    return data.map(lambda r: [r[-1], Vectors.dense(r[:-1])]).toDF(['label','features'])\n",
    "\n",
    "vectorized_CV_data = vectorizeData(stratified_CV_data)\n",
    "\n",
    "# Index labels, adding metadata to the label column\n",
    "labelIndexer = StringIndexer(inputCol='label',\n",
    "                             outputCol='indexedLabel').fit(vectorized_CV_data)\n",
    "\n",
    "# Automatically identify categorical features and index them\n",
    "featureIndexer = VectorIndexer(inputCol='features',\n",
    "                               outputCol='indexedFeatures',\n",
    "                               maxCategories=2).fit(vectorized_CV_data)\n",
    "\n",
    "# Train a DecisionTree model\n",
    "dTree = DecisionTreeClassifier(labelCol='indexedLabel', featuresCol='indexedFeatures')\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dTree])\n",
    "\n",
    "# Search through decision tree's maxDepth parameter for best model\n",
    "paramGrid = ParamGridBuilder().addGrid(dTree.maxDepth, [2,3,4,5,6,7]).build()\n",
    "\n",
    "# Set F-1 score as evaluation metric for best model selection\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='indexedLabel',\n",
    "                                              predictionCol='prediction', metricName='f1')    \n",
    "\n",
    "# Set up 3-fold cross validation\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "CV_model = crossval.fit(vectorized_CV_data)\n",
    "\n",
    "# Fetch best model\n",
    "tree_model = CV_model.bestModel.stages[2]\n",
    "print tree_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation\n",
    "vectorized_test_data = vectorizeData(final_test_data)\n",
    "\n",
    "transformed_data = CV_model.transform(vectorized_test_data)\n",
    "print evaluator.getMetricName(), 'accuracy:', evaluator.evaluate(transformed_data)\n",
    "\n",
    "predictions = transformed_data.select('indexedLabel', 'prediction', 'probability')\n",
    "predictions.toPandas().head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 accuracy: 0.843460594844"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
